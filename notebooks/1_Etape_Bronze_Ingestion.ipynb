{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Couche Bronze – Ingestion des Données Brutes**\n",
    "Ce notebook ingère le jeu de données brut *Online Retail II* à partir d’un volume géré dans le Lakehouse Databricks.  \n",
    "Il utilise Pandas pour lire le fichier Excel, ajoute des métadonnées d’audit, et sauvegarde les données brutes au format Delta sous forme de table `bronze_sales` pour un traitement ultérieur dans la couche Silver."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install openpyxl to enable reading Excel files with pandas\n",
    "%pip install openpyxl\n",
    "# Step 1: Importing pandas to read the Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: definition of the full path to the uploaded Excel file in your volume\n",
    "file_path = \"/Volumes/workspace/default/ecommerce_demo/online_retail_II.xlsx\"\n",
    "\n",
    "# Step 3: Read the Excel file using pandas\n",
    "pdf = pd.read_excel(file_path)\n",
    "\n",
    "# Step 4: Show the first 5 rows of data\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Étape 2 : Conversion en DataFrame Spark\n",
    "\n",
    "Le fichier Excel brut a été chargé avec succès dans une DataFrame Pandas (`pdf`).  \n",
    "Il est maintenant converti en DataFrame Spark (`df_raw`) afin de permettre le traitement distribué des données avec Apache Spark.  \n",
    "Cette transformation prépare le jeu de données pour les opérations ETL en aval et son stockage au format Delta Lake dans la couche Bronze."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert all columns to string type to avoid mixed-type conversion issues\n",
    "pdf_clean = pdf.astype(str)\n",
    "\n",
    "# Now safely convert to Spark DataFrame\n",
    "df_raw = spark.createDataFrame(pdf_clean)\n",
    "\n",
    "# Preview the result\n",
    "df_raw.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Étape 3 : Ajouter une colonne d’audit d’ingestion\n",
    "\n",
    "Pour assurer la traçabilité des données, une colonne supplémentaire appelée `ingested_at` est ajoutée à l’ensemble de données.  \n",
    "Cette colonne enregistre la date et l’heure exactes de l’ingestion des données dans l’environnement Databricks.  \n",
    "Inclure ce type de métadonnées est une bonne pratique standard dans les pipelines de données pour faciliter le débogage, le versionnement et l’historisation."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Add an ingestion timestamp column to the Spark DataFrame\n",
    "df_bronze = df_raw.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "# Display the result with the audit column\n",
    "df_bronze.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✏️ Étape 3.1 : Ajuster les noms de colonnes pour la compatibilité Delta\n",
    "\n",
    "Pour se conformer aux contraintes techniques de Delta Lake, les noms de colonnes contenant des espaces ou des caractères spéciaux ont été ajustés.  \n",
    "Plus précisément, les espaces sont remplacés par des underscores (ex. `Customer ID` → `Customer_ID`). Cela garantit la compatibilité avec les règles de validation de schéma de Delta Lake.\n",
    "\n",
    "Il est important de noter qu’aucune valeur des données n’a été modifiée au cours de ce processus.  \n",
    "L’intégrité des données brutes est préservée tout en les rendant compatibles avec le format Delta."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Step: Clean column names by replacing spaces with underscores\n",
    "df_bronze = df_bronze.toDF(*[col.replace(\" \", \"_\") for col in df_bronze.columns])\n",
    "\n",
    "# Display to confirm updated column names\n",
    "df_bronze.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Étape 4 : Sauvegarde sous forme de table Delta Bronze\n",
    "\n",
    "La DataFrame Spark, désormais enrichie d’une colonne `ingested_at`, est sauvegardée de manière persistante au format Delta dans une table nommée `bronze_sales`.  \n",
    "Cela marque l’achèvement de la couche Bronze, dans laquelle les données brutes sont capturées telles quelles pour des raisons d’audit et de traçabilité.  \n",
    "En utilisant Delta Lake, les données bénéficient des fonctionnalités de contrôle de version, de validation de schéma et de Time Travel."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the final DataFrame as a Delta Table (Bronze layer)\n",
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_sales\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "notebookName": "01_bronze_ingestion"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

